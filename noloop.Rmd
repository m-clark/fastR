---
title: <span style="font-size:150%; font-variant:small-caps; font-style:italic">FastR</span>
author: |
  <div class="title"><span style="font-size:125%; font-variant:small-caps; font-style:normal">[Michael Clark](http://m-clark.github.io)</span><br><br>
  [<img src="../img/CSCAR_darkletter.png", width='50%'>](http://cscar.research.umich.edu/)</div>
  
output: 
  html_document:
    css: notebooks.css
    highlight: pygments
    toc: yes
    toc_float: yes
  html_notebook:
    #css: notebooks.css
    highlight: pygments
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment=NA, cache=T, message = F, warnings=F, 
                      R.options=list(width=120))
```


```{r loadlibs, echo=FALSE, cache=FALSE}
library(microbenchmark);library(readr); library(dplyr); library(data.table); library(feather)
```


<br><br><br>

# Introduction

This document provides ways to speed up your code *before* parallelization on your own machine or cluster.  There are many ways to use R in an optimal fashion, but often these are not taught in applied statistics courses or perhaps are not otherwise presented in a cohesive way.  In other cases, one may simply be new to R, or coming at it from a programming language that works differently.  

The 'R is slow' mantra is an old one, and has never been entirely accurate in my opinion for two reasons. First, even if it is slower, and it *is* relative to many languages for many situations, the time saved in programming will far exceed any computational time lost.  R often can do things in 1 or 2 lines of code that will require custom functions to be written in other languages, and in that sense R is one of the fastest languages out there for statistical analysis. This is usually the case for data processing/manipulation as well.  R was designed expressly for statistical analysis, not for computational speed.  For more on some of this point see [Hadley Wickham's chapter](http://adv-r.had.co.nz/Performance.html).

Secondly, there are simply faster ways to use R, and they often involve a programming approach that is different from what one would use in say, Python or other languages.  That is the focus of this document.

For the timings we'll be using the <span class="pack">microbenchmark</span> package, which can show speed differences for even extremely fast operations.  This way we don't have to engage in necessarily slow operations to show the differences well.  You'll need to install it if you want to make your own comparisons. Note also that timings will be in microseconds, milliseconds, or seconds.

- To be added
    - data structures/representations (alternatives to data.frames)
        - data.table, dplyr, dtplyr, multidplyr
        - [pooling](https://github.com/rstudio/pool)
        - sparklyr

Color coding:


<span class="emph">emphasis</span>

<span class="pack">package</span>

<span class="func">function</span>

<span class="objclass">object/class</span>

[link]()

# Faster Data Input/Output

The base R functions read.csv and similar work fine for smaller data sets.  Given their flexibility they may be preferred when useful. However with larger data sets they can be too slow and/or use too much memory. One has plenty of alternatives though.

## readr

<span class="pack">readr</span> is a newer package and provides many complements of the base R functions, substituting an \_ for the . of commonly used file reading functions. For example:

- <span class="func">read_delim</span>: read delimited files
    - read_csv: comma delimited
    - read_tsv: tab delimited
- <span class="func">read_lines</span>: read a file line by line
- <span class="func">read_file</span>: read a whole file in as a string (e.g. for text processing)

```{r createBigData, echo=FALSE, cache=FALSE}
write_csv(expand.grid(nums=rnorm(1e4), lets=letters), 'data/testreadr.csv')
write_csv(expand.grid(nums=rnorm(1e5), lets=letters), 'data/testchunk.csv')
```


To show the difference, we have a simple data set of two columns, one numeric (random normal), and one string (letters). It is large enough for demonstration, but still not very big (less than 6MB on disk).

```{r showBigData, echo=FALSE}
test = read.csv('data/testreadr.csv')
str(test)
```

Here is a comparison reading the the file in with both <span class="func">readr</span> and <span class="func">read.csv</span>.


```{r readcsv, echo=FALSE}
suppressMessages({
microbenchmark(readr = read_csv("data/testreadr.csv", progress = F),
               baser = read.csv("data/testreadr.csv"), times=20)
})
```

The <span class="pack">readr</span> package makes assumptions about the column type based on the first 1000 observations, but one can override this.  As mentioned, for small data the standard function is fine, and even faster by default.

```{r readcsvSmall, echo=F}
suppressMessages({
microbenchmark(readr = read_csv(readr_example("mtcars.csv")),
               baser = read.csv(readr_example("mtcars.csv")))
})
```


### Chunked 

The <span class="pack">readr</span> package has chunked versions of those same functions, so that not all the data has to be read in at the same time.  This is extremely important for data processing.  While one may not have as much choice once the analysis stage is reached, many data processing tasks can be chunked or be done line by line, thereby opening doors for parallelization and memory control.

```{r readrChunk, echo=FALSE}
f = function(d, pos) dplyr::filter(d, lets=='a')

suppressMessages({
microbenchmark(readr = dplyr::filter(read_csv('data/testchunk.csv', progress = FALSE), lets=='a'),
               chunk = read_csv_chunked('data/testchunk.csv',  DataFrameCallback$new(function(d,pos) dplyr::filter(d, lets=='a')), chunk_size=1e6, progress = F), times = 20)
})
```


## data.table

The <span class="pack">data.table</span> package also has a read function that will amount to faster read speeds. The function <span class="func">fread</span> works much like the previous functions to detect column types quickly and otherwise has optimizations in place to speed the process.  Both <span class="func">read_*</span> and <span class="func">fread</span> might be viable depending on the situation.  Wickham [notes](https://github.com/hadley/readr) the following comparison:

<div style="font-size:90%">
- Compared to fread, readr:
 
- Is slower (currently ~1.2-2x slower. If you want absolutely the best performance, use data.table::fread().

- Readr has a slightly more sophisticated parser, recognising both doubled ("""") and backslash escapes ("\""). Readr allows you to read factors and date times directly from disk.

- fread() saves you work by automatically guessing the delimiter, whether or not the file has a header, how many lines to skip by default and more. Readr forces you to supply these parameters.

- The underlying designs are quite different. Readr is designed to be general, and dealing with new types of rectangular data just requires implementing a new tokenizer. fread() is designed to be as fast as possible. fread() is pure C, readr is C++ (and Rcpp).</div>





I will note that <span class="pack">readr</span> was faster on the 54 Mb chunk test file, so you may need very large files before <span class="func">fread</span> starts to shine.

```{r dtChunk, echo=FALSE, eval=FALSE}
library(data.table)
suppressMessages({
microbenchmark(readr = read_csv('data/testchunk.csv', progress = F),
               fread = fread('data/testchunk.csv'), times =20)
})
# read_csv('data/testchunk.csv', progress = F)
# fread('data/testchunk.csv')
```

## Lines vs. Whole Data

Many do not realize that they don't have to read in the whole data file.  Many preprocessing steps can be done line by line, and this may save on memory and provide a means of (massively) parallel operations.  Also, and here is a point further not considered by many, this provides a means to debug your code before going prime time.  Your file may be huge, but you don't have to work with the whole thing to get your code sorted out.  See <span class="func">readLines</span> in base R for details, but you'd want to use <span class="func">read_lines</span> in <span class="pack">readr</span>.


## Other

### feather

Yet another alternative is to use a different data format altogether.  Wickham and Wes McKinney (the latter is behind Python's <span class="pack">pandas</span> module) are working on <span class="pack">feather</span>, a binary data format that would be read in R *or* Python, and allow for very fast read/write operations, as well as smaller file sizes, and more interoperability between the two languages.  See [this link](https://blog.rstudio.org/2016/03/29/feather/) for more details.  The following compares times using feather vs. the <span class="pack">readr</span> counterparts.

```{r featherTest, echo=FALSE}
library(feather)
microbenchmark(write_csv = write_csv(expand.grid(nums=rnorm(1e4), lets=letters), 'data/testreadr.csv'),
               write_feather = write_feather(expand.grid(nums=rnorm(1e4), lets=letters), 'data/testfeather.feather'), times=20)
microbenchmark(read_csv = read_csv('data/testreadr.csv', progress = F),
               read_feather = read_feather('data/testfeather.feather'), times=20)
```

# Data Structures

In progress, skip this section...
While data.frames are probably the most commonly used data structures in R, they are very slow to work with.  

Packages dplyr, data.table, dtplyr


From Bennet re: flux
> For Flux, one thing that should be added is to use data on local disk rather
than a network volume wherever possible.  For many small files, this can 
increase speed by a factor of 3 or more with not other changes.  For large 
files, it can vary from twice as fast as unbusy, high-speed network storage to 
more than ten times as fast if the storage in on a hammered, slow NFS server, or
going over a slow network connection with many hops.


# Loops and Alternatives

## Loops

Before getting to alternative methods, we'll first do things with a standard loop, and then show others options later.  Loops in and of themselves are not a problem in R, nor are they necessarily slow for some tasks. It's just that they just often have more efficient alternatives, whether it be more succinct code, or possibly vastly quicker computation.

As one of our working examples, we'll start with a function that does a random walk. The code is based on [this document](https://www.stat.auckland.ac.nz/~ihaka/downloads/Taupo-handouts.pdf), co-authored by one of the R's in R, with some slight additions and modifications. 

```{r loop_nopre}
rw2d_loop_nopre <- function(n) {
  xpos = ypos = 0
  xdir = c(T, F)
  pm1 = c(1,-1)
  
  for (i in 2:n)
  {
    if (sample(xdir, 1)) {
      xpos[i] = xpos[i-1] + sample(pm1, 1)
      ypos[i] = ypos[i-1]
    }
    else {
      xpos[i] = xpos[i-1]
      ypos[i] = ypos[i-1] + sample(pm1, 1)
    }
  }
  data.frame(x=xpos, y=ypos) 
}

# rw2d_loop_nopre(1e5) # for profiling
# ggplot2::qplot(x, y, data=rw2d_loop_nopre(1000), geom='path') # plot check
```
```{r plotloop1, echo=FALSE}
ggplot2::qplot(x, y, data=rw2d_loop_nopre(1000), geom='path', col=I('orange')) + lazerhawk::theme_trueMinimal() # plot check
```


For better understanding of what's going on, you can use the debug functionality on the functions used to see what they are doing line by line. Rstudio makes this especially easy, and to get started one can use code that looks like the following.

```{r debug, eval=FALSE}
debugonce(rw2d_loop_nopre)
rw2d_loop_nopre(1000)
```

You will be able to see what's produced and even manipulate those objects at each step.


### Pre-allocation

One thing you'll want to note, if you must use an explicit loop, first create an empty vessel to contain the elements of the loops output, rather than taking an append-as-you-go approach.  The reasoning is similar to the reasoning behind [vectorization](#vectorization) noted later.  By not having R do some (internal) operations every single iteration, you save time, and it adds up. Memory is also already made available to the object.

In the following we'll compare the random walk both with and without pre-allocation. I have a line for each function so you can use them for <span class="emph">profiling</span>, to see where bottlenecks arise specifically.  In (newer versions of) RStudio, you can simply highlight that line and hit Ctrl+Alt+Shft+P to do so. Also, for the first demonstration above I showed an example graph, but all others will look like some (random) version of that, so are subsequently commented out in the code.

```{r loop_pre}
# preallocation
rw2d_loop_pre <- function(n) {
  xpos = ypos = numeric(n)
  xdir = c(T, F)
  pm1 = c(1,-1)
  
  for (i in 2:n)
  {
    if (sample(xdir, 1)) {
      xpos[i] = xpos[i-1] + sample(pm1, 1)
      ypos[i] = ypos[i-1]
    }
    else {
      xpos[i] = xpos[i-1]
      ypos[i] = ypos[i-1] + sample(pm1, 1)
    }
  }
  data.frame(x=xpos, y=ypos)
}

# rw2d_loop_pre(1e5) # for profiling
# ggplot2::qplot(x, y, data=rw2d_loop_pre(1000), geom='path')
```

```{r preAlloBench, echo=FALSE}
microbenchmark(rw2d_loop_nopre(1000), rw2d_loop_pre(1000))
```

As we can see, even with this quick example pre-allocation is already saving some time.  

### Smaller code != faster

Note that the goal is fast running code, but this doesn't mean there is a one-to-one correlation between speed and code size.  The following is a recursive version of the above and it's three lines of code.  It is a little faster than the non-pre-allocated loop, but slower than one with pre-allocation.  I might also mention that R is not really that great for recursive functions in my experience, often 'nesting too deeply' where other languages have no issue.

```{r recursive}
rw2d2_recursive <- function(n, x=0, y=0) {
  if(n == 1) return(cumsum(data.frame(x=x, y=y)))
  step = sample(1:4, 1)
  rw2d2_recursive(n=n-1, 
                  x=c(x, c(-1, 1, 0, 0)[step]), 
                  y=c(y, c(0, 0, -1, 1)[step]))
}

# cannot be profiled
# ggplot2::qplot(x, y, data=rw2d2_recursive(1000), geom='path')
```

```{r loopsCompare, echo=FALSE}
microbenchmark(rw2d_loop_nopre(1000),
               rw2d_loop_pre(1000),
               rw2d2_recursive(1000))
```

Looks can be deceiving!

### Double loops

Double loops for independent operations are almost always an inefficient way to use R, especially for common data processing tasks, so if you find yourself using one, you should probably think very hard about what alternatives might be possible.  In this example we create a distance matrix based on the Euclidean distance between rows in a given matrix `m`, which for timing demonstration will be a 100 x 100 matrix.

```{r dloops, cache=TRUE}
doubleLoopDist <- function(m) {
  n = nrow(m)
  dists = matrix(0, ncol=n, nrow=n)
  for (i in 1:n){
    for (j in 1:n){
      dists[i,j] = sqrt(sum((m[i,] - m[j,])^2))
    }
  }
  dists
}


d = matrix(rnorm(10000), ncol=100)
# doubleLoopDist(d)
```

As we'll see in a moment, this is very inefficient, and I would again say there is rarely a need to a double loop.  Even if you have to double loop, there might be an efficiency gain possible.  Since the distance matrix is symmetric, we only need to compute half of it, then fill in the rest. This would at least cut the time in half.

```{r dloop2}
doubleLoopDist2 <- function(m) {
  n = nrow(m)
  dists = matrix(0, ncol=n, nrow=n)
  for (i in 1:(n-1)){
    for (j in (i+1):n){
      dists[i,j] = sqrt(sum((m[i,] - m[j,])^2))
    }
  }
  
  dists = dists + t(dists) + diag(diag(dists))
  diag(dists) = 0
  dists
}

identical(doubleLoopDist(d), doubleLoopDist2(d))
```

```{r echo=FALSE}
microbenchmark(dloop1 = doubleLoopDist(d), dloop2 = doubleLoopDist2(d))
```

Later on we'll see just how slow this is when we compare it to the base R <span class="func">dist</span> function, which uses compiled C code.

## Apply

A set of functions exists in R that typically make explicit loops unnecessary, and can reduce programming time, and potentially make code cleaner. Furthermore, they are parallelizable, and so potentially far faster.  There is a misconception that these functions are faster than loops out-of-the-box, but this is incorrect, and they technically can even be slower.  Again though, you automatically gain in coding efficiency and possibly gain a lot in computation time with the use of parallelization.

### apply functions

- <span class="func">apply</span>: arrays, matrices, data.frames
- <span class="func">lapply</span>, <span class="func">sapply</span>, <span class="func">vapply</span>: lists, data.frames, vectors
- <span class="func">tapply</span>: grouped operations (table apply)
- <span class="func">mapply</span>: multivariate version of sapply
- <span class="func">replicate</span>: similar to sapply

As a simple example, let's estimate some column means. An explicit loop might look like the following.

```{r colMeansLoop}
means_loop = rep(0, ncol(d))  # initialize the means vector
for (n in 1:ncol(d)){
  means_loop[n] = mean(d[,n])
}
head(means_loop)
```

```{r colMeansLoopFunc, echo=F}
# because echoing specific lines doesn't work with notebooks apparently
means_loop_func <- function(d) {
  means = rep(0, ncol(d))
  for (n in 1:ncol(d)){
    means[n] = mean(d[,n])
  }
  means
}
```

Let's use the <span class="func">apply</span> function instead.  The primary arguments are a matrix/data.frame (or any array), the margin to iterate over (e.g. rows or columns), and the function we want to apply, which in this case is the mean.

```{r  colMeansApply}
means_apply = apply(d, 2, mean)
identical(means_loop, means_apply)
```

And that's all there is to it.  A point of emphasis however: *there is no guaranteed speed gain here*, only more succinct code. The following demonstrates using a loop vs. apply on a 1000x1000 matrix. For a simple operation like this, the explicit loop is actually faster.  However, most would not want to write a loop every time they want column or row means, and as we'll see later, we'd use a vectorized approach rather than either of these.

```{r colMeansCompare, echo=F}
x = matrix(rnorm(1e6), ncol=1e3)
microbenchmark(means_loop = means_loop_func(x), 
               means_apply = apply(x, 2, mean))
```


### plyr alternates

The <span class="pack">plyr</span> package provides alternatives to the apply family of functions. These plyr functions work based on the first two letters. The first denotes the input, the second the output.  For example, <span class="func">ldply</span> takes a <span class="objclass">list</span> and converts the output to a <span class="objclass">data.frame</span>.

- Lists: <span class="func">ldply</span>, <span class="func">llply</span>, <span class="func">laply</span>

- Arrays: <span class="func">a*ply</span>

- Dataframes: <span class="func">d*ply</span>

- Multiple arguments: <span class="func">m*ply</span>

As an example will use <span class="func">adply</span> on the column mean example.  The <span class="objclass">d</span> object is a <span class="objclass">matrix</span>, and we'll get a <span class="objclass">data.frame</span> as output.

```{r plyr}
library(plyr)
adply(d, 2, mean) %>% head
```

Unfortunately, the flexibility does come at a price, and <span class="pack">plyr</span> functions are very slow (mostly because *<span class="objclass">data.frames</span> are slow*).  Using the 1000x1000 matrix again, and using <span class="func">aaply</span> to be more comparable, we see the <span class="pack">plyr</span> function is much slower.

```{r plyrComparison, echo=FALSE}
microbenchmark(means_loop = means_loop_func(x), 
               means_apply = apply(x, 2, mean),
               aaply = aaply(x, 2, mean))
```

However, there is comparable functionality for <span class="objclass">data.frames</span> in <span class="pack">dplyr</span> that would be much faster.


### Parallel versions

Another thing that puts the apply approach ahead of loops regards parallelization.  Most of the base R apply functions have par* counterparts, e.g. parApply, parLapply etc.  This is also the case for plyr functions, which have a `.parrallel=TRUE` argument.  Example code would like the following.

```{r parDemo, eval=FALSE}
x = matrix(rnorm(1e7), ncol=1e6)
library(parallel)
cl = makeCluster(8)
clusterExport(cl, 'x')
means = parApply(cl, x, 2, mean)
stopCluster(cl)
```

For the plyr functions, one needs to use the foreach package and register the cluster, and supply the `.parallel` argument.  It's still exceedingly slow, so you might just stick with the parallel/foreach package.  

```{r parPlyr, eval=FALSE}
library(foreach); library(doParallel)
library(parallel)
cl = makeCluster(12)
registerDoParallel(cl)
means = aaply(x, 2, mean, .parallel = T, .paropts = list(.export='x'))
stopCluster(cl)
```


```{r multidplyr, eval=FALSE, echo=FALSE}
library(multidplyr)
```


For something simple like this, there isn't going to be much gained in going parallel. And this is an important point to note: *using more cores does not necessarily mean more speed*, and this is especially the case when spreading operations across machines.  Had I used 12 instead of 8 on my machine, the difference would have been negligible.  I should also note that none of these approaches is nearly as fast as we could get with a built-in R function, <span class="func">colMeans</span>.

```{r parallelComparison, echo=FALSE, eval=FALSE}
parallelComparison = microbenchmark(means_loop = means_loop_func(x), 
                                    means_apply = apply(x, 2, mean),
                                    parallelMeans = parApply(cl, x, 2, mean), times=10)
save(parallelComparison, file='data/parallelComparison.RData')
stopCluster(cl)
```


```{r loadParallelComparison, echo=FALSE}
load('data/parallelComparison.RData')
parallelComparison
```


## Using faster languages

### Base R internals

Much of base R is not written in R, but much faster languages such as C, C++ and Fortran, and using those functions can lead to enormous speed gains.  Relative to an explicit loop that does the same thing, there is no comparison to a base R function that's calling internal C or other code.  Not only will the built-in function run faster, but your code will be more succinct because you're making a single function call as opposed to an explicit loop.

As an example let's compare the previous double loop approach to the base R function <span class="func">dist.</span>

```{r dist, echo=FALSE}
microbenchmark(dloop = doubleLoopDist(d), 
               dloop2=doubleLoopDist2(d), 
               dist= dist(d, diag = T, upper = T))
```


If there are actually only two columns though, dist would be slower due to some preliminary checks and calculations that <span class="func">sum</span> and <span class="func">sqrt</span> functions do not have to make (note that these timings are in **micro**seconds). 

```{r distfor2col, echo=FALSE}
microbenchmark(dist= dist(d[,1:2]), sqrtsum = sqrt(sum((d[,1] - d[,2])^2)))
```

Again, when there isn't much to optimize, don't waste time figuring out how to do so.

> Premature optimization is the root of all evil. <br><br>
> \~ Donald Knuth


As another example, let's use the <span class="func">colMeans</span> function on that large 10 x 1000000 matrix we used before.

```{r colmeans, echo=FALSE}
rbind(parallelComparison, microbenchmark(colMeans = colMeans(x)))
```

This shows that using the compiled C in the <span class="func">colMeans</span> function is way faster than even parallelizing (in this case).

As a further illustration, let's center a matrix, i.e. subtract the mean. We now have several options at this point, but we'll keep it to the following:

- using the <span class="func">scale</span> function with the argument `scale = FALSE`
- using the <span class="func">sweep</span> function
- subtract the column means calculated via <span class="func">colMeans</span>

```{r, echo=FALSE}
microbenchmark(means_sweep = sweep(x, 2, STATS=colMeans(x), check.margin=F),
               scale = scale(x, scale=F),
               subtract_colMeans = x - colMeans(x))
```

The interesting thing to note here is that <span class="func">sweep</span> is already as fast as the built in <span class="func">scale</span> function, but we could have provided any statistic of the columns to 'sweep out'. For example, maybe we prefer the median, or perhaps we want to add rather than subtract. The slowdown of <span class="func">sweep</span> is that the statistic to be swept out may require a slower and/or separate operation (e.g. I could first create an object <span class="objclass">means</span> that are the column means). In the end, one might have several options depending on their goals.


### Rcpp

We can potentially take advantage of the power of C++ explicitly within R, by using the Rcpp package. It's also worth noting that many R packages do so to pick up some speed.

```{r Rcpp, cache=F}
library(Rcpp)

distC = cppFunction('
NumericMatrix distC(NumericMatrix x) {
  int n = x.nrow();
  NumericMatrix out(n, n);

  for(int i = 0; i < n; ++i) {
    for (int j = 0; j < n; ++j) {
      out(i,j) = sqrt(sum(pow(x(i,_) - x(j,_), 2.0)));
    }
  }
  return out;
}
')
distC(d[1:3,])
dist(d[1:3,], diag = T, upper = T)
```

Now that we can see thse both produce the same thing, let's time the <span class="func">dist</span> function, the C++ version and the loop.

```{r RcppCompare, cache=T, echo=FALSE}
microbenchmark(dist= dist(d), 
               distC = distC(d),
               dloop = doubleLoopDist(d))
```


Note that in bigger matrices the R function would eventually win out, but the point here is that we're at least in the ballpark. Furthermore, like most languages, it is easy enough to run another language script from within that program.  So if you're working within another language for the heavy lifting, you could call R from it, or vice versa, i.e. write what you need in another language and call it from R.



## Vectorization

<span class="emph">Vectorization</span> is a key concept in R, though some coming to R from other languages (e.g. Matlab) may be familiar with it. The basic idea is that we pass whole vectors to operations rather than the elements of it, and as such R can treat the vector once and go from there, rather than treat each element of the vector as a unique snowflake that must be considered individually, and figuring things out every time the operation is run.

As an example let's do a simple loop vs. a vectorized approach.

```{r vectorization}
x = 1:3
y = 7:9
for (i in seq_along(x)) {
  print(x[i] + y[i])
}
```

Every iteration, R evaluates what type of object x[i] is, what y[i] is, whether adding them is possible etc. If we use a vectorized approach, R figures this out once and proceeds accordingly.

```{r vectorization2}
x + y
```


### Vectorizing a sequential operation

Let's return to our random walk example for some benchmarks. This is a nice example because we can see that sometimes even seemingly sequential operations might be put in a form that would not require a loop.  The first vectorized example uses no explicit loop but gives us the same result. It uses <span class="func">ifelse</span>, which is a vectorized version of the `if... else` construct.  However, <span class="func">ifelse</span> comes with some baggage, and <span class="pack">dplyr</span> provides a faster version, <span class="func">if_else</span>.

```{r vectorizedFunctions}
# vectorize
rw2d2_vec_ifelse <- function(n) {
  steps = sample(c(-1, 1), n-1, replace=T)
  xdir = sample(c(T, F), n-1, replace=T)
  xpos = c(0, cumsum(ifelse(xdir, steps, 0)))
  ypos = c(0, cumsum(ifelse(xdir, 0, steps)))
  data.frame(x=xpos, y=ypos)
}

# rw2d2_vec_ifelse(1e7)
# ggplot2::qplot(x, y, data=rw2d2_vec_ifelse(1000), geom='path')


# dplyr if_else can be faster if there are many operations
rw2d2_vec_ifelse2 <- function(n) {
  steps = sample(c(-1, 1), n-1, replace=T)
  xdir = sample(c(T, F), n-1, replace=T)
  xpos = c(0, cumsum(if_else(xdir, steps, 0)))
  ypos = c(0, cumsum(if_else(xdir, 0, steps)))
  data.frame(x=xpos, y=ypos)
}

# rw2d2_vec_ifelse2(1e7)
# ggplot2::qplot(x, y, data=rw2d2_vec_ifelse2(1000), geom='path')
```

However, we can still get more efficient. Here we do all the necessary sampling for the x and y positions first, then subsequently fill out those positions. We also just use the fact that <span class="func">cumsum</span> can work on the <span class="objclass">data.frame</span> itself.  As <span class="objclass">data.frames</span> are <span class="objclass">lists</span>, <span class="func">cumsum</span> will work on its elements, which in this case are the columns.

```{r vectorization_noifelse}
rw2d2_vec_noifelse <- function(n) {
  dir = sample(1:4, n-1, replace=T)
  xpos = c(0, c(-1, 1, 0, 0)[dir])
  ypos = c(0, c(0, 0, -1, 1)[dir])
  cumsum(data.frame(x=xpos, y=ypos))
}

# rw2d2_vec_noifelse(1e7)
# ggplot2::qplot(x, y, data=rw2d2_vec_noifelse(1000), geom='path')
```


Now we compare all the looping choices considered thus far:

- loop no pre-allocation
- loop with pre-allocation
- vectorization (2 versions)
- more efficient vectorization

The last version is succinct in code and much faster as well.

```{r loop_vectorization_comparison, echo=FALSE}
microbenchmark(rw2d_loop_nopre(10000),
               rw2d_loop_pre(10000),
               rw2d2_vec_ifelse(10000),
               rw2d2_vec_ifelse2(10000),
               rw2d2_vec_noifelse(10000))

# ggplot2::autoplot(
# microbenchmark(rw2d_loop_nopre(1000),
#                rw2d_loop_pre(1000),
#                rw2d2_vec_ifelse(1000),
#                rw2d2_vec_noifelse(1000),
#                rw2d2_recursive(1000))
# )
```


For more examples and issues with vectorization see the following:

[A very accessible intro to vectorization](http://www.noamross.net/blog/2014/4/16/vectorization-in-r--why.html) Noam Ross

[When to leave loops alone](http://adv-r.had.co.nz/Functionals.html#functionals-not) Hadley Wickham

[R Inferno (3rd Circle)](http://www.burns-stat.com/pages/Tutor/R_inferno.pdf) Patrick Burns



# Matrix operations

Consider a linear combination to produce model predictions as in a standard regression model. We multiply each column by a coefficient and sum them.


```{r lincom}
b = runif(11) # first value is the intercept

X = matrix(rnorm(100000), ncol=10)

lincom1 = b[1] + b[2]*X[,1] + b[3]*X[,2] + b[4]*X[,3] + b[5]*X[,4] + b[6]*X[,5] + 
  b[7]*X[,6] + b[8]*X[,7] + b[9]*X[,8] + b[10]*X[,9] + b[11]*X[,10]
```

While this is vectorized, we can do better. We can use matrix multiplication to produce the same result.  In addition, the biggest gain here is programming time.  Matrix multiplication takes far less to code up.

```{r lincomMatrix}
X = cbind(1, X) # add intercept column
lincom2  = X %*% b
```

However, even within matrix operations there might be faster ways to do it. In this case, <span class="func">crossprod</span> is slightly faster.

```{r lincomCrossprod}
lincom3 = tcrossprod(X, t(b))

identical(lincom1, c(lincom2))
identical(lincom2, lincom3)
```

Let's time them.

```{r lincomTime, echo=FALSE}
microbenchmark(lc1 = b[1] + b[2]*X[,1] + b[3]*X[,2] + b[4]*X[,3] + b[5]*X[,4] + b[6]*X[,5] + b[7]*X[,6] + b[8]*X[,7] + b[9]*X[,8] + b[10]*X[,9] + b[11]*X[,10],
               lc2 = X %*% b,
               lc3 = tcrossprod(X, t(b)))
```


They all produce identical output, but using a matrix operation is over 4 times as fast as the simple sum, and crossprod over 11 times as fast.  Programming time is quicker also.  


Even how we do the matrix operations can have an effect. The following examples use various means to obtain coefficients from a standard linear regression model.  We'll start with just the base R <span class="func">lm</span> function.  The next is the <span class="func">lm.fit</span> function, which is what lm uses, but after a lot of pre- and post-processing of the data.  After that we use basic matrix multiplication, a grouped version of it, followed by use of the <span class="func">crossprod</span> function.

```{r matrixLM}
set.seed(1234)
n = 10000000                   # sample size
x = rnorm(n)                   # covariate
y =  5 + 2*x + rnorm(n)        # target
X = cbind(1, x)                # model.matrix


# lm
lm_base  = lm(y ~ x)

# use the underlying fit function
lm_fit  = lm.fit(X, y)          # note: qr.solve isn't really a gain here but could also be used

# an even faster version
lm_fit2  = .lm.fit(X, y)

# normal equations to get coefficients
matrix_raw = solve(t(X) %*% X) %*% t(X) %*% y

matrix_grouped = solve(t(X)%*%X) %*% (t(X) %*% y)

# crossprod approach
matrix_crossprod = crossprod(solve(crossprod(X)), crossprod(X,y))


cbind(cbind(lm_base=coef(lm_base), 
      lm_fit=coef(lm_fit), 
      lm_fit2=coef(lm_fit2), 
      matrix_raw=matrix_raw[,1],
      matrix_grouped=matrix_grouped[,1],
      matrix_crossprod=matrix_crossprod[,1]))
```


```{r matrixlmComparisonsConstruct, echo=FALSE, eval=FALSE}
# add se? sqrt(diag(crossprod(test$residuals)[,1]/n * solve(crossprod(X))))
matrixlm = microbenchmark(lm_base  = lm(y ~ x),
                          lm_fit  = lm.fit(X, y),
                          # qr = qr.solve(X,y),
                          lm_fit2  = .lm.fit(X, y),
                          matrix_raw = solve(t(X) %*% X) %*% t(X) %*% y,
                          matrix_grouped = solve(t(X)%*%X) %*% (t(X) %*% y),
                          matrix_crossprod = crossprod(solve(crossprod(X)), crossprod(X,y)), times=20)

save(matrixlm, file='data/matrixlm.RData')
```

Now for the timings.

```{r matrixlmComparisonsDisplay, echo=FALSE, eval=TRUE}
load('data/matrixlm.RData')
matrixlm
```

We can see above that these approaches all produce the same results, but take very different amounts of time to complete. We get a significant gain if we reduce the <span class="func">lm</span> overhead, where <span class="func">lm.fit</span> is almost 10x faster, and the <span class="func">.lm.fit</span>, which is just a wrapper for the C code call, is faster still. We can still get faster by using matrix operations, and even get a slight gain if we group them a certain way.  This reduces memory usage as well.  The idea is that if we go from `solve(t(X)%*%X)` and post multiply by `t(X)`, we're dealing with a 2 x 10^7^ matrix. If we first `t(X) %*% y`, we're now just multiplying a 2x2 matrix with a 2x1 matrix. In the end though, crossprod is the most optimized, and notably faster than any of the other approaches.


And finally, there are efficient methods, particularly for <span class="emph">sparse</span> matrices, in packages such as <span class="pack">Matrix</span>. For dense matrices it has the same speed as standard R, but would still have other within-package functionality that might be useful.  

```{r MatrixPackage}
library(Matrix)
X2 = as(X, 'dgeMatrix')
y2 = as(y, 'dgeMatrix')
```

```{r evalMatrixPackage, echo=F}
microbenchmark(matrix_crossprod = crossprod(solve(crossprod(X)), crossprod(X,y)),
               matrix_crossprod2 = crossprod(solve(crossprod(X2)), crossprod(X2,y2)))
```

Here's a comparison with sparse matrices, where memory as well as speed gains can be made. The base R object is 200 times larger.

```{r sparseMatrix}
m <- matrix(0, 500, 500)
m[runif(314, 0, length(m))] <- 1
mm <- as(m, "dgCMatrix")
object.size(m) / object.size(mm) 
```

The speed gains are notable as well.

```{r compareCrossprods}
microbenchmark(sparse_tcrossprod <- tcrossprod(mm),
               sparse_crossprod <- crossprod(mm),
               base_crossprod <- crossprod(m),
               base_tcrossprod <- tcrossprod(m))
```


As a final point, let's return to the distance matrix computation.  We saw that using Rcpp could get us about as fast as the base R function.  It used a double loop, but we could also use RcppEigen and a matrix approach to create a crossproduct function, along with a different algorithm for calculating.  Here's the following code from the article on RcppEigen by [Bates and Eddelbuettel](http://cran.irsn.fr/web/packages/RcppEigen/vignettes/RcppEigen-Introduction.pdf).  I use the inline package to compile the string to C++ as in their example.

The C++ code uses the Eigen library to create a matrix crossproduct (for one matrix) for reals (there is also a commented out line for the tcrossprod equivalent), so in the demo I show equivalence to R's crossprod by converting integers to numeric class.

```{r rcppMatrix}
crossprodCPP = '
  using Eigen::Map;
  using Eigen::MatrixXd;
  using Eigen::Lower;
  
  const Map<MatrixXd>   A(as<Map<MatrixXd> >(AA));
  const int             m(A.rows()), n(A.cols());
  
   MatrixXd              AtA(MatrixXd(n, n).setZero().selfadjointView<Lower>().rankUpdate(A.adjoint()));
  // MatrixXd              AAt(MatrixXd(m, m).setZero().selfadjointView<Lower>().rankUpdate(A));

  return List::create(Named("crossprod(A)") = AtA);
'

library(inline)
fcprd = cxxfunction(signature(AA='matrix'), crossprodCPP, 'RcppEigen')
fcprd(matrix(as.numeric(1:9), 3))
crossprod(matrix(as.numeric(1:9), 3))
```

Now we create the distance function that uses that crossproduct function once, then does the rest in base R.[^betterdist]

```{r distMatCpp2}
edmcpp <- function(d, rows=F) {
  if(rows) d= t(d)
  ones = matrix(rep(1, ncol(d)), ncol=1) 
  xtx = fcprd(d)[[1]]
  edm =  sqrt(tcrossprod(ones, diag(xtx)) + tcrossprod(diag(xtx), ones) - 2*xtx)
  edm
}
```

Now for the timings.  We can compare it to the base R crossprod and the distance functions we tested previously.

```{r}
microbenchmark(crossprod(m),
               fcprd(m))

microbenchmark(dist(d),
               distC(d),
               edmcpp(d, rows = T))
```

In both cases we witness about a 4 to 5-fold speedup.

# Other R builds

Other versions of R exist. For those using university high performance computing, the R build you are using will be different, for example.  Here is a run down of some, partly taken from Wickham's chapter linked in the introduction of this document. There are others, but I just note the ones that still appear to be in regular development.

[Microsoft Open R](https://mran.revolutionanalytics.com/open/):  From Microsoft, specifically the former Revolution Analytics group. While the build is typically a point release behind the current version of R, that won't affect most users, and it's easy to install on one's own machine just like any R.

[pqR](http://www.pqr-project.org/) (pretty quick R): by Radford Neal. Fixes many obvious performance issues, and provides better memory management and some support for automatic multithreading.

[Renjin](http://www.renjin.org/):  by BeDataDriven. Renjin uses the Java virtual machine, and has an extensive test suite.



# Summary

To complete the partial quote referred to earlier...

> "Programmers waste enormous amounts of time thinking about, or worrying about, the speed of noncritical parts of their programs, and these attempts at efficiency actually have a strong negative impact when debugging and maintenance are considered. We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil. Yet we should not pass up our opportunities in that critical 3%." <br><br>
\~ Donald Knuth

There are no good benchmarks for the balance of programming vs. computational time for statistical analysis and related activities. However, the reason R will beat other languages hands down in many cases is because it is so much easier to program certain, often very common operations. Furthermore, the functionality available across a wide variety of modeling paradigms is unmatched at present. Knowing just a few 'tricks' such as those demonstrated here will help gain those small efficiencies in the 97% time.  In the end, what approach works best will depend on the specifics of the task, but you will always have options with R, and often pretty fast ones.



# References

Two excellent texts on R programming are:

[The Art of R Programming](https://www.nostarch.com/artofr.htm) by Norm Matloff

[Advanced R](http://adv-r.had.co.nz/) by Hadley Wickham.

[^betterdist]: I hope to return and extend this to the entire distance calculation using RcppEigen.